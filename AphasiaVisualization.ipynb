{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of BlaBla linguistic features for aphasia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D t-SNE visualization of linguistic features of fluent/nonfluent aphasia sufferers and healthy controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig, path_base, close=False):\n",
    "    file_path = f'{path_base}.png'\n",
    "    fig.savefig(file_path, bbox_inches='tight')\n",
    "    if close:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not provide public access to the following files to protect patient privacy. For more information about accessing AphasiaBank data, see [this page](https://aphasia.talkbank.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the English AphasiaBank features calculated using BlaBla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_ab = pd.read_csv('features_aphasiabank_english.csv')\n",
    "df_eng_ab = df_eng_ab.dropna()\n",
    "df_eng_ab['task'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_ab = df_eng_ab[df_eng_ab['task'] == 'Cinderella']\n",
    "df_eng_ab.group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandarin data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Mandarin AphasiaBank features calculated using BlaBla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man_ab = pd.read_csv('features_aphasiabank_mandarin.csv')\n",
    "df_man_ab = df_man_ab.dropna()\n",
    "df_man_ab = df_man_ab.replace({\n",
    "    'normal': 'control',\n",
    "    'aa': 'anomic',\n",
    "    'ca': 'conduction',\n",
    "    'tma': 'transmotor',\n",
    "})\n",
    "df_man_ab['task'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man_ab = df_man_ab[df_man_ab['task'] == 'Cry_Wolf']\n",
    "df_man_ab.group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the French AphasiaBank features calculated using BlaBla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fre_ab = pd.read_csv('features_aphasiabank_french.csv')\n",
    "df_fre_ab = df_fre_ab.dropna()\n",
    "df_fre_ab['task'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fre_ab = df_fre_ab[df_fre_ab['task'] == 'Cinderella']\n",
    "df_fre_ab.group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two different granularities of groupings for both English and on a cross-lingual basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disease groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_a(row):\n",
    "    if row.group in ['wernicke', 'anomic', 'conduction', 'broca', 'transmotor', 'global', 'aphasia']:\n",
    "        return 'aphasia'\n",
    "    if row.group in ['control']:\n",
    "        return 'control'\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_b(row):\n",
    "    if row.group in ['wernicke', 'anomic', 'conduction']:\n",
    "        return 'fluent_aphasia'\n",
    "    if row.group in ['broca', 'transmotor', 'global']:\n",
    "        return 'nonfluent_aphasia'\n",
    "    if row.group in ['control']:\n",
    "        return 'control'\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_a_lang(row, lang):\n",
    "    prefix = lang + '_'\n",
    "    if row.group in ['wernicke', 'anomic', 'conduction', 'broca', 'transmotor', 'global', 'aphasia']:\n",
    "        return prefix+'aphasia'\n",
    "    if row.group in ['control']:\n",
    "        return prefix+'control'\n",
    "    return prefix+'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_b_lang(row, lang):\n",
    "    prefix = lang + '_'\n",
    "    if row.group in ['wernicke', 'anomic', 'conduction']:\n",
    "        return prefix+'fluent_aphasia'\n",
    "    if row.group in ['broca', 'transmotor', 'global']:\n",
    "        return prefix+'nonfluent_aphasia'\n",
    "    if row.group in ['control']:\n",
    "        return prefix+'control'\n",
    "    return prefix+'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_ab['group_a'] = df_eng_ab.apply(lambda row: get_group_a_lang(row, 'english'), axis=1)\n",
    "df_eng_ab['group_b'] = df_eng_ab.apply(lambda row: get_group_b_lang(row, 'english'), axis=1)\n",
    "\n",
    "df_man_ab['group_a'] = df_man_ab.apply(lambda row: get_group_a_lang(row, 'mandarin'), axis=1)\n",
    "df_man_ab['group_b'] = df_man_ab.apply(lambda row: get_group_b_lang(row, 'mandarin'), axis=1)\n",
    "\n",
    "df_fre_ab['group_a'] = df_fre_ab.apply(lambda row: get_group_a_lang(row, 'french'), axis=1)\n",
    "df_fre_ab['group_b'] = df_fre_ab.apply(lambda row: get_group_b_lang(row, 'french'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab = pd.concat([df_eng_ab, df_man_ab, df_fre_ab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab['group_a'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab['group_b'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot disease t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot BlaBla features for the fluent aphasia vs nonfluent aphasia vs healthy control granularity grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sns.color_palette('muted')\n",
    "all_features = [\n",
    "    'noun_rate',\n",
    "    'verb_rate',\n",
    "    'demonstrative_rate',\n",
    "    'adjective_rate',\n",
    "    'pronoun_rate',\n",
    "    'adverb_rate',\n",
    "    'conjunction_rate',\n",
    "    'possessive_rate',\n",
    "    'noun_verb_ratio',\n",
    "    'noun_ratio',\n",
    "    'pronoun_noun_ratio',\n",
    "    'prop_close_class_words',\n",
    "    'prop_open_class_words',\n",
    "    'content_density',\n",
    "    'idea_density',\n",
    "    'honore_statistic',\n",
    "    'brunet_index',\n",
    "    'type_token_ratio'\n",
    "    ,'mean_word_length',\n",
    "    'prop_inflected_verbs',\n",
    "    'prop_auxiliary_verbs',\n",
    "    'prop_gerund_verbs',\n",
    "    'prop_participle_verbs',\n",
    "    'num_clauses',\n",
    "    'num_clauses_per_sentence',\n",
    "    'prop_nouns_with_det',\n",
    "    'prop_nouns_with_adjectives',\n",
    "    'num_noun_phrases',\n",
    "    'noun_phrases_rate',\n",
    "    'num_verb_phrases',\n",
    "    'verb_phrases_rate',\n",
    "    'num_infinitive_phrases',\n",
    "    'infinitive_phrases_rate',\n",
    "    'num_prepositional_phrases',\n",
    "    'prepositional_phrases_rate',\n",
    "    'num_dependent_clauses',\n",
    "    'dependent_clauses_rate',\n",
    "    'max_yngve_depth',\n",
    "    'mean_yngve_depth',\n",
    "    'total_yngve_depth',\n",
    "    'const_pt_height',\n",
    "    'num_discourse_markers',\n",
    "    'discourse_markers_rate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(data, col_for_labels, col_map, label_map, perplexity=30, n_iter=2000, exclusion_list=[]): \n",
    "    # Exclude any features in the exclusion list.\n",
    "    selected_columns = [x for x in all_features if x not in exclusion_list]\n",
    "\n",
    "    # Normalize the data and calculate the t-SNE projection.\n",
    "    data_np = data[selected_columns]\n",
    "    data_np = normalize(data_np, axis=0)\n",
    "    tsne = TSNE(n_components=2, verbose=0, perplexity=perplexity, n_iter=n_iter, n_jobs=-1)\n",
    "    tsne_results = tsne.fit_transform(data_np)\n",
    "\n",
    "    # Plot the t-SNE by group.\n",
    "    groups = pd.DataFrame(tsne_results, columns=['x', 'y']).assign(category=data[col_for_labels].values).groupby('category')\n",
    "    fig = plt.figure(figsize=(4, 4), dpi=300)\n",
    "    \n",
    "    paths, legend_texts = [], []\n",
    "    for label in label_map.keys():\n",
    "        points = groups.get_group(label)\n",
    "        print(label, len(points.x))\n",
    "        paths.append(plt.scatter(points.x, points.y, label=label_map[label], c=[col_map[label][0]], alpha=col_map[label][1], lw=0, s=40))\n",
    "        legend_texts.append(label_map[label])\n",
    "\n",
    "    plt.axis('off')\n",
    "    \n",
    "    figlegend_h = plt.figure(figsize=(4,2), dpi=300)\n",
    "    figlegend_h.legend(paths, legend_texts, 'center', ncol=10) \n",
    "    \n",
    "    figlegend_v = plt.figure(figsize=(4,2), dpi=300)\n",
    "    figlegend_v.legend(paths, legend_texts, 'center', ncol=1) \n",
    "    \n",
    "    return fig, figlegend_h, figlegend_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, _, figlegend_v = plot_tsne(df_eng_ab, 'group_b',\n",
    "               col_map={\n",
    "                   'english_fluent_aphasia': (cols[0], 0.6),\n",
    "                   'english_nonfluent_aphasia': (cols[4], 0.6),\n",
    "                   'english_control': (cols[1], 0.6),\n",
    "               },\n",
    "               label_map={\n",
    "                   'english_fluent_aphasia': 'Fluent aphasia',\n",
    "                   'english_nonfluent_aphasia': 'Nonfluent aphasia',\n",
    "                   'english_control': 'Controls',\n",
    "               })\n",
    "\n",
    "save_fig(fig, 'disease_tsne_group_b')\n",
    "save_fig(figlegend_v, 'disease_tsne_group_b_vlegend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot language t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To promote cross-task/-language generalizability, exclude features which are undefined in Mandarin and those which scale ~linearly with speech length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [\n",
    "    'demonstrative_rate',\n",
    "    'infinitive_phrases_rate',\n",
    "    'num_infinitive_phrases',\n",
    "    'prop_gerund_verbs',\n",
    "    'prop_inflected_verbs',\n",
    "    'prop_participle_verbs',\n",
    "    'num_verb_phrases',\n",
    "    'num_clauses',\n",
    "    'num_clauses_per_sentence',\n",
    "    'num_dependent_clauses',\n",
    "    'num_discourse_markers',\n",
    "    'num_noun_phrases',\n",
    "    'num_prepositional_phrases',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _, figlegend_v = plot_tsne(df_ab, 'group_a', perplexity=30, exclusion_list=exclusion_list,\n",
    "               col_map={\n",
    "                   'english_aphasia': (cols[0], 0.6),\n",
    "                   'english_control': (cols[1], 0.6),\n",
    "                   'mandarin_aphasia': (cols[2], 0.6),\n",
    "                   'mandarin_control': (cols[3], 0.6),\n",
    "                   'french_aphasia': (cols[4], 0.6),\n",
    "                   'french_control': (cols[5], 0.6),\n",
    "               },\n",
    "               label_map={\n",
    "                   'english_aphasia': 'English aphasia',\n",
    "                   'english_control': 'English controls',\n",
    "                   'mandarin_aphasia': 'Mandarin aphasia',\n",
    "                   'mandarin_control': 'Mandarin controls',\n",
    "                   'french_aphasia': 'French aphasia',\n",
    "                   'french_control': 'French controls',\n",
    "               })\n",
    "\n",
    "save_fig(fig, 'language_tsne_group_a')\n",
    "save_fig(figlegend_v, 'language_tsne_group_a_vlegend')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
